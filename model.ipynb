{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls ../../..","metadata":{"id":"lw5hWsmdB5QT","outputId":"ab5f7346-5ce5-4e77-b180-63da700a9742","_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-object-detection-api","metadata":{"id":"xtRMpbszCIN5","outputId":"1ddb8857-2dba-4a30-e74f-b030a08f9892","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"id":"0_BVaIP_CkVG","outputId":"2b13af1e-9fe4-44eb-f9b6-9de53cdb1c5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models","metadata":{"id":"M0lWrLwVCooU","outputId":"91d46975-40a1-4338-989a-39beb8c1ed8b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n!tar -xvf ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n!rm ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz","metadata":{"id":"6Zh5JTKyFJ8r","outputId":"8c9834df-8dad-4572-e243-749a608faf98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/models/research\n!protoc object_detection/protos/*.proto --python_out=.\n# Install TensorFlow Object Detection API.\n!cp object_detection/packages/tf2/setup.py .\n!python -m pip install .\n!pip install tensorflow==2.13.0","metadata":{"id":"L6WGacceIRAG","outputId":"ee182f19-e191-4ea3-f780-d3cf10863bdf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/models/research/object_detection/builders/model_builder_tf2_test.py","metadata":{"id":"6Uc2JSq6IaQI","outputId":"2aa081dd-ab0a-432c-b3ad-c1c16297b314","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wget","metadata":{"id":"aaLKjpJXIhZ0","outputId":"cbe506ca-b067-4ae7-ea9d-296a3610de5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.config\n!mv ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.config /kaggle/working/ssd_resnet101.config","metadata":{"id":"ypYQ1Pk1Inng","outputId":"042481a4-da15-45da-fd7b-70975d9ea2a3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 12\nbatch_size = 4\nnum_steps = 20000\nnum_eval_steps = 1000\n\ntrain_record_path = '/kaggle/input/tfrecord/train.tfrecord'\ntest_record_path = '/kaggle/input/tfrecord/test.tfrecord'\nmodel_dir = '/kaggle/working/training/'\nlabelmap_path = '/kaggle/input/labelmap/labelmap.pbtxt'\n\npipeline_config_path = '/kaggle/working/ssd_resnet101.config'\nfine_tune_checkpoint = '/kaggle/working/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nwith open(pipeline_config_path) as f:\n    config = f.read()\n\nwith open(pipeline_config_path, 'w') as f:\n\n    # Set labelmap path\n    config = re.sub('label_map_path: \".*?\"',\n                  'label_map_path: \"{}\"'.format(labelmap_path), config)\n\n    # Set fine_tune_checkpoint path\n    config = re.sub('fine_tune_checkpoint: \".*?\"',\n                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n\n    # Set fine_tune_checkpoint typr\n    config = re.sub('fine_tune_checkpoint_type: \".*?\"',\n                  'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n\n    # Set train tf-record file path\n    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")',\n                  'input_path: \"{}\"'.format(train_record_path), config)\n\n    # Set test tf-record file path\n    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")',\n                  'input_path: \"{}\"'.format(test_record_path), config)\n\n    # Set number of classes.\n    config = re.sub('num_classes: [0-9]+',\n                  'num_classes: {}'.format(num_classes), config)\n\n    # Set batch size\n    config = re.sub('batch_size: [0-9]+',\n                  'batch_size: {}'.format(batch_size), config)\n\n    # Set training steps\n    config = re.sub('num_steps: [0-9]+',\n                  'num_steps: {}'.format(num_steps), config)\n    \n    # Set learning rate\n   # config = re.sub('learning_rate_base: [.0-9]+',\n    #              'learning_rate_base: {}'.format(learning_rate), config)\n\n    f.write(config)","metadata":{"id":"Iho0suoMKRn0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Hapus seluruh blok konfigurasi optimizer momentum\nconfig = re.sub(r'optimizer\\s*{[^}]*}', '', config)\n\n    # Pengaturan optimizer Adam yang akan digunakan\nrms_prop_optimizer_config = \"\"\"\n    optimizer {\n      rms_prop_optimizer: {\n        learning_rate: {\n          cosine_decay_learning_rate {\n            learning_rate: 0.001  # Sesuaikan dengan kebutuhan Anda\n          }\n        }\n      }\n      use_moving_average: false\n    }\n    \"\"\"\n\n    # Gabungkan pengaturan optimizer Adam dengan konfigurasi yang ada\nconfig += rms_prop_optimizer_config\n\nprint(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={pipeline_config_path} \\\n    --model_dir={model_dir} \\\n    --alsologtostderr \\\n    --num_train_steps={num_steps} \\\n    --sample_1_of_n_eval_examples=1 \\\n    #--checkpoint_every_n=100 \\\n    --num_eval_steps={num_eval_steps}","metadata":{"id":"4Uv2-zruJ1u6","outputId":"23526dbd-ad30-4efe-857e-a32fba700aad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VALIDATION","metadata":{"id":"yZyIW2gkiDn8"}},{"cell_type":"code","source":"!python /kaggle/working/models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={pipeline_config_path} \\\n    --model_dir={model_dir} \\\n    --checkpoint_dir={model_dir}","metadata":{"id":"2u4SQOjjiFNt","outputId":"85be44f9-b979-47a4-ebae-74f593318ebe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_directory = '/kaggle/working/inference_graph'\n\n!python /kaggle/working/models/research/object_detection/exporter_main_v2.py \\\n    --trained_checkpoint_dir={model_dir} \\\n    --output_directory={output_directory} \\\n    --pipeline_config_path={pipeline_config_path}","metadata":{"id":"rM6Opm4diIGu","outputId":"c8edbfcd-eb24-41e2-b4c5-3f295d2eff95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TFLITE**","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/models/research/object_detection/export_tflite_graph_tf2.py \\\n    --pipeline_config_path={pipeline_config_path} \\\n    --trained_checkpoint_dir /kaggle/working/training \\\n    --output_directory /kaggle/working/inference_graph_tflite","metadata":{"id":"mG9pjeU_iLTx","outputId":"91372a84-8e2c-4f51-f8fb-e4ad5062470e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/inference_graph_tflite\nimport tensorflow as tf\n\nconverter = tf.lite.TFLiteConverter.from_saved_model('/kaggle/working/inference_graph_tflite/saved_model') # path to the SavedModel directory\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\ntflite_model = converter.convert()\n# Save the TensorFlow Lite model to a file\nwith open('model17.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/inference_graph.zip /kaggle/working/inference_graph/\n!zip -r /kaggle/working/inference_graph_tflite.zip /kaggle/working/inference_graph_tflite/\n!zip -r /kaggle/working/training.zip /kaggle/working/training/","metadata":{"id":"kXr1hH7dkBaI","outputId":"e293e578-4975-4228-b81f-5a23b6e51803","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TENSORBOARD**","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip ./ngrok-stable-linux-amd64.zip\n!./ngrok authtoken 2WR6sz0LL5J47kwf2UYMNr1L1fG_7AGSSvhLnEBtEGNvyz8Bw #ganti authken e signup ng ngrok","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:42:27.902757Z","iopub.execute_input":"2023-10-12T11:42:27.903172Z","iopub.status.idle":"2023-10-12T11:42:32.056997Z","shell.execute_reply.started":"2023-10-12T11:42:27.903139Z","shell.execute_reply":"2023-10-12T11:42:32.055643Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Launch and tunnel tensorboard\nimport os\nimport multiprocessing\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir /kaggle/working/training --host 0.0.0.0 --port 6009 &\",\n                        \"./ngrok http 6009 &\"\n                        ]]","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:42:34.445842Z","iopub.execute_input":"2023-10-12T11:42:34.446264Z","iopub.status.idle":"2023-10-12T11:42:35.013146Z","shell.execute_reply.started":"2023-10-12T11:42:34.446226Z","shell.execute_reply":"2023-10-12T11:42:35.010516Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ntime.sleep(10) # wait for tensorboard host\n! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:42:44.270210Z","iopub.execute_input":"2023-10-12T11:42:44.270867Z","iopub.status.idle":"2023-10-12T11:42:55.495449Z","shell.execute_reply.started":"2023-10-12T11:42:44.270828Z","shell.execute_reply":"2023-10-12T11:42:55.493735Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****TESTING****","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom collections import namedtuple\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds","metadata":{"id":"amqp5VcMjZ3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/hugozanini/object-detection/master/inferenceutils.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\nsys.path.insert(0, '/kaggle/working/')\n\nfrom inferenceutils import *","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_directory = 'inference_graph/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)\ntf.keras.backend.clear_session()\nmodel = tf.saved_model.load(f'/kaggle/working/{output_directory}/saved_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing Image**","metadata":{}},{"cell_type":"code","source":"images = [\"0156.jpg\", \"0240.jpg\", \"0422.jpg\"]\n\nfor image_name in images:\n    image_np = load_image_into_numpy_array('/kaggle/input/testing/' + image_name)\n    output_dict = run_inference_for_single_image(model, image_np)\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        image_np,\n        output_dict['detection_boxes'],\n        output_dict['detection_classes'],\n        output_dict['detection_scores'],\n        category_index,\n        instance_masks = output_dict.get('detection_masks_reframed', None),\n        use_normalized_coordinates = True,\n        min_score_thresh = .46,\n        line_thickness = 6)\n    display(Image.fromarray(image_np))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing Video**","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport tensorflow as tf\nimport cv2\nimport numpy as np\n\nfrom object_detection.utils import label_map_util\nfrom PIL import Image\n#from google.colab.patches import cv2_imshow\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n#Path to saved model\n\nPATH_TO_SAVED_MODEL = \"/kaggle/working/inference_graph/saved_model\"\n\n# Load label map and obtain class names and ids\n#label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategory_index=label_map_util.create_category_index_from_labelmap(\"/kaggle/input/images2/images/labelmap.pbtxt\",use_display_name=True)\nnum_detect = [0,0,0,0]\n\ndef visualise_on_image(image, bboxes, labels, scores, thresh):\n    (h, w, d) = image.shape\n    for bbox, label, score in zip(bboxes, labels, scores):\n        if score > thresh:\n            xmin, ymin = int(bbox[1]*w), int(bbox[0]*h)\n            xmax, ymax = int(bbox[3]*w), int(bbox[2]*h)\n\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0,255,0), 2)\n            cv2.putText(image, f\"{label}: {int(score*100)} %\", (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n            if label == \"R02\":\n              num_detect[0] += 1\n              cv2.putText(image, f\"1: {num_detect[0]}\", (50,70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (182, 3, 252), 2)\n            if label == \"R03\":\n              num_detect[1] += 1\n              cv2.putText(image, f\"2: {num_detect[1]}\", (60,70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (182, 3, 252), 2)\n            if label == \"L00\":\n              num_detect[2] += 1\n              cv2.putText(image, f\"3: {num_detect[2]}\", (70,70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (182, 3, 252), 2)\n            if label == \"00\":\n              num_detect[3] += 1\n              cv2.putText(image, f\"4: {num_detect[3]}\", (80,70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (182, 3, 252), 2)\n            #print(label)\n            #print(type(label))\n    return image\n\nif __name__ == '__main__':\n\n    # Load the model\n    print(\"Loading saved model ...\")\n    detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n    print(\"Model Loaded!\")\n\n    # Video Capture (video_file)\n    video_capture = cv2.VideoCapture(\"/kaggle/input/videotesting/output_test.avi\")\n    start_time = time.time()\n\n    frame_width = int(video_capture.get(3))\n    frame_height = int(video_capture.get(4))\n    #fps = int(video_capture.get(5))\n    size = (frame_width, frame_height)\n\n    #Initialize video writer\n    result = cv2.VideoWriter('/kaggle/working/output_test.avi', cv2.VideoWriter_fourcc(*'MJPG'),15, size)\n\n    while True:\n      ret, frame = video_capture.read()\n      if not ret:\n          print('Unable to read video / Video ended')\n          break\n\n      frame = cv2.flip(frame, 1)\n      image_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n      # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n      # The model expects a batch of images, so also add an axis with `tf.newaxis`.\n      input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n\n      # Pass frame through detector\n      detections = detect_fn(input_tensor)\n\n      # Set detection parameters\n\n      score_thresh = 0.4   # Minimum threshold for object detection\n      max_detections = 100\n\n      # All outputs are batches tensors.\n      # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n      # We're only interested in the first num_detections.\n      scores = detections['detection_scores'][0, :max_detections].numpy()\n      bboxes = detections['detection_boxes'][0, :max_detections].numpy()\n      labels = detections['detection_classes'][0, :max_detections].numpy().astype(np.int64)\n      labels = [category_index[n]['name'] for n in labels]\n\n      # Display detections\n      visualise_on_image(frame, bboxes, labels, scores, score_thresh)\n\n      end_time = time.time()\n      fps = int(1/(end_time - start_time))\n      start_time = end_time\n      cv2.putText(frame, f\"FPS: {fps}\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n      #cv2_imshow(frame)\n      #print ([category_index.get(value) for index,value in enumerate(labels[0]) if scores[0] > 0.4])\n      #Write output video\n      result.write(frame)\n\n    for i in num_detect:\n        print(i, end=\" \")\n    video_capture.release()","metadata":{},"execution_count":null,"outputs":[]}]}